<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>/* Build Your First Artificial Neuron in C: The Perceptron Made Simple */ | ~# cr1s1um > /dev/random</title><meta name=keywords content="Programming languages,C,AI,Machine Learning,Neural Network,LLM,Large Language Model"><meta name=description content="Learn how to build a perceptron in C: your first artificial neuron explained step by step, the foundation of neural networks and machine learning."><meta name=author content="Vincenzo Argese"><link rel=canonical href=https://cr1s1um.github.io/en/post/perceptron-c-artificial-neuron-first-step/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://cr1s1um.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://cr1s1um.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://cr1s1um.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://cr1s1um.github.io/apple-touch-icon.png><link rel=mask-icon href=https://cr1s1um.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://cr1s1um.github.io/en/post/perceptron-c-artificial-neuron-first-step/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-XHH0DSS4T3"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XHH0DSS4T3")}</script><meta property="og:url" content="https://cr1s1um.github.io/en/post/perceptron-c-artificial-neuron-first-step/"><meta property="og:site_name" content="~# cr1s1um > /dev/random"><meta property="og:title" content="/* Build Your First Artificial Neuron in C: The Perceptron Made Simple */"><meta property="og:description" content="Learn how to build a perceptron in C: your first artificial neuron explained step by step, the foundation of neural networks and machine learning."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2025-08-09T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-09T00:00:00+00:00"><meta property="article:tag" content="Programming Languages"><meta property="article:tag" content="C"><meta property="article:tag" content="AI"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Neural Network"><meta property="article:tag" content="LLM"><meta name=twitter:card content="summary"><meta name=twitter:title content="/* Build Your First Artificial Neuron in C: The Perceptron Made Simple */"><meta name=twitter:description content="Learn how to build a perceptron in C: your first artificial neuron explained step by step, the foundation of neural networks and machine learning."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://cr1s1um.github.io/en/post/"},{"@type":"ListItem","position":2,"name":"/* Build Your First Artificial Neuron in C: The Perceptron Made Simple */","item":"https://cr1s1um.github.io/en/post/perceptron-c-artificial-neuron-first-step/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"/* Build Your First Artificial Neuron in C: The Perceptron Made Simple */","name":"\/* Build Your First Artificial Neuron in C: The Perceptron Made Simple *\/","description":"Learn how to build a perceptron in C: your first artificial neuron explained step by step, the foundation of neural networks and machine learning.","keywords":["Programming languages","C","AI","Machine Learning","Neural Network","LLM","Large Language Model"],"articleBody":"Perceptron for Simulating an AND Logic Gate Idea This code was developed as a practical exercise based on a YouTube video by @enkk.\nIn the video, Enkk explains in a simple and intuitive way what a perceptron is, using a practical and easily understandable example. In this post, we implement the perceptron code.\nGitHub link to the implementation Useful Links: Needless to say, following @Enkk is highly recommended if you are interested in studying Large Language Models (LLMs) and AI.\nEnkk’s YouTube Channel Video “how it works: neural networks (pt.1)” Wikipedia - Perceptron The Automatic Gate Analogy The example used in the video is particularly effective: imagine a parking system where a car can enter only if both gates (Gate A and Gate B) are open at the same time… OBVIOUSLY! ;-)\nLogical Scheme of the Example Perceptron System Logic (Example from @Enkk): Gate A open = signal 1 Gate B open = signal 1 Output 1 = The car may enter Output 0 = The car may not enter Truth Table (AND): Gate A Gate B Result Action 0 (closed) 0 (closed) 0 ❌ Car does not enter 1 (open) 0 (closed) 0 ❌ Car does not enter 0 (closed) 1 (open) 0 ❌ Car does not enter 1 (open) 1 (open) 1 ✅ Car may enter Connection with Digital Logic This analogy perfectly represents the behavior of an AND logic gate, where the output is 1 only when both inputs are 1. In all other cases, the output is 0.\nWhy Use a Perceptron? It’s important to note that implementing a simple AND gate requires only a few lines of code:\nint porta_and(int a, int b) { return a \u0026\u0026 b; // Very simple! } …even simpler:\n#include int main(void){ int a,b; a=0; // Assign value 1 or 0 b=0; // Assign value 1 or 0 if (a \u0026\u0026 b){ printf(\"Valore output: 1\\n\"); } else{ printf(\"Valore output: 0\\n\"); } } What is a Perceptron? The perceptron is the simplest and most fundamental form of an artificial neural network. It is a single artificial neuron that makes decisions based on the data it receives as input.\nBasic Structure A perceptron is made up of:\nInputs: The data it receives (in our case, the state of the two gates) Weights: Numbers that determine the importance of each input (VERY IMPORTANT: weights are the parameters of a Large Language Model, i.e., its intelligence) Bias: A threshold value that helps with the final decision Activation Function: Decides whether to “activate” the neuron (output 1) or not (output 0) Output: The final decision (1 or 0) Why is the perceptron the smallest element of a Neural Network? The perceptron is like the “brick” used to build more complex neural networks:\nSimple Neural Network: 1 perceptron or a few perceptrons (nowadays only for didactic purposes) Multilayer Neural Network: Hundreds or thousands of perceptrons connected together Deep Learning: Millions of perceptrons organized into layers Just as you need to understand how a single brick works before building a house, to understand neural networks it’s useful to first master the perceptron.\nWhy is it Important to Understand How It Works? Theoretical Foundation: All advanced concepts (backpropagation, gradient descent, etc.) are derived from perceptron principles. Didactic Simplicity: Simple enough to fully understand, but contains all the key concepts and allows you to build a mental model. Historical Basis: It was the first artificial neuron model. Intuitive Understanding: Once you understand the perceptron, the conceptual leap to more complex networks becomes much more natural. Didactic Value of This Exercise Using a perceptron for this example may seem excessive, but it represents an excellent educational exercise for several reasons:\nSimple Problem: The AND gate is easy to visualize. Verifiable Result: It’s easy to check whether it works (I added printouts in the code step by step). Complete Concepts: Includes all fundamental elements (training, weights, bias, epochs). Conceptual Bridge: Links basic digital logic to advanced machine learning. 1. Understanding the Fundamentals of Neural Networks Weights: How the network assigns importance to inputs Bias: The threshold term that influences the final decision Activation Function: How binary outputs are produced 2. Learning Process Training Epochs: Repeated learning cycles Dataset: Collection of input-output examples for training Weight Updates: How the network “learns” from its mistakes (VERY IMPORTANT) 3. Learning Algorithm Forward Pass: Prediction calculation Error Calculation: Comparing with the expected output Backward Pass: Updating parameters based on error 4. Convergence Concepts How the network reaches an optimal solution Stopping criteria for training Performance evaluation Implementation NOTE: This is version 1 of the implementation. I will probably update the code on GitHub as I come up with new ideas. Always refer to the GitHub project for the latest version.\nGitHub link to the implementation\n/* * PERCEPTRON * (AND Logic Gate Learning) * * Author: Vincenzo Argese * Web: https://cr1s1um.github.io/ * Date: 2025-08-09 * Version: v1.0 * Idea: Youtuber @Enkk video “come funziona: le reti neurali (pt.1)” https://www.youtube.com/watch?v=2UdQQA65jcM * * Description: * This program implements a simple perceptron (single artificial neuron) that learns * to simulate an AND logic gate through supervised learning. * * The perceptron receives two binary inputs (0 or 1) and learns to output: * - 1 only when both inputs are 1 (like an AND gate) * - 0 in all other cases * * Training process: * 1. Initialize weights and bias with small random values * 2. For each training example, calculate prediction * 3. Compare prediction with expected output * 4. If wrong, adjust weights using perceptron learning rule * 5. Repeat until all predictions are correct or max epochs reached * * This demonstrates fundamental neural network concepts: weights, bias, * activation functions, training epochs, and supervised learning. */ #include #define EPOCHS 100 // Define constant: maximum number of training epochs /* Forward declaration of activation function */ int activation_function(float sum); int main(void) { /* Training Dataset */ int x[4][2]={ // Input matrix: 4 examples with 2 features each {0,0}, // First example: x1=0, x2=0 -\u003e expected output: 0 {1,0}, // Second example: x1=1, x2=0 -\u003e expected output: 0 {0,1}, // Third example: x1=0, x2=1 -\u003e expected output: 0 {1,1} // Fourth example: x1=1, x2=1 -\u003e expected output: 1 }, y[4]={0,0,0,1}; // Expected outputs: represents AND logic function /* Weights and Bias initialization */ float w1 = 0.1, // Weight for first input, initialized to 0.1 w2 = 0.1, // Weight for second input, initialized to 0.1 b = 0.1; // Bias term, initialized to 0.1 // Learning rate parameter float learning_rate = 0.1; // Controls how much weights are adjusted during learning /* Training Phase */ for (int epoch=0; epoch \u003c EPOCHS; epoch++){ // Main training loop: up to 100 epochs int errors = 0; // Counter for errors in current epoch // Process all training examples for(int i=0; i\u003c4; i++){ // Iterate through all 4 examples in dataset // DEBUG - Print current training state printf(\"TRAINING EPOCH: %d\\n\", epoch); // Fixed typo: epoch instead of ephoc printf(\"Current errors: %d\\n\", errors); // Print current error count printf(\"Input X1: %d - X2: %d\\n\", x[i][0], x[i][1]); // Print current inputs printf(\"Expected output Y: %d\\n\", y[i]); // Print expected output printf(\"w1: %.1f - w2: %.1f - Bias: %.1f\\n\", w1, w2, b); // Print current weights and bias printf(\"Learning Rate: %.1f\\n\", learning_rate); // Print learning rate // Forward pass: calculate weighted sum float weighted_sum = (x[i][0]*w1 + x[i][1]*w2) + b; // Linear combination: x1*w1 + x2*w2 + bias int predicted_output = activation_function(weighted_sum); // Apply activation function // DEBUG - Print forward pass results printf(\"Weighted sum: %.1f\\n\", weighted_sum); // Print the weighted sum printf(\"Predicted output: %d\\n\", predicted_output); // Print network's prediction // Calculate prediction error int error = y[i] - predicted_output; // Error = expected - predicted // DEBUG - Print error information printf(\"Prediction error: %d\\n\", error); // Print the error printf(\"---------------------------------------------\\n\"); // Separator line // Update weights if there's an error (perceptron learning rule) if (error != 0){ // Only update if prediction is wrong // Apply perceptron learning rule w1 += learning_rate * error * x[i][0]; // Update w1: w1 = w1 + η * error * x1 w2 += learning_rate * error * x[i][1]; // Update w2: w2 = w2 + η * error * x2 b += learning_rate * error; // Update bias: b = b + η * error errors++; // Increment error counter } } // Check convergence: stop if no errors occurred if(errors == 0){ // Perfect classification achieved printf(\"TRAINING COMPLETED - epoch: %d\\n\", epoch); // Print completion message break; // Exit training loop early } } // Print final results printf(\"\\n\\n\"); // Print empty lines for readability printf(\"=== FINAL TRAINED PARAMETERS ===\\n\"); // Print results header printf(\"Final w1: %.1f - w2: %.1f - Bias: %.1f\\n\", w1, w2, b); // Print final weights and bias // Test the trained perceptron printf(\"\\n=== TESTING TRAINED PERCEPTRON ===\\n\"); for(int i=0; i\u003c4; i++){ float test_sum = (x[i][0]*w1 + x[i][1]*w2) + b; int test_output = activation_function(test_sum); printf(\"Input (%d,%d) -\u003e Output: %d (Expected: %d)\\n\", x[i][0], x[i][1], test_output, y[i]); } return 0; } /* * Activation Function * * Purpose: Converts continuous weighted sum to binary output * Parameter: float sum - the weighted sum from perceptron * Returns: 1 if sum \u003e 0, otherwise returns 0 * * This implements a threshold activation function commonly used in perceptrons */ int activation_function(float sum){ return (sum \u003e 0) ? 1 : 0; // Ternary operator: if sum \u003e 0 return 1, else return 0 } Conclusion An AND gate can be implemented in a much simpler way, but this exercise provides a solid foundation for understanding the fundamental mechanisms behind more complex neural networks. It serves as an ideal bridge between basic digital logic and advanced machine learning concepts.\nEvery element that may seem “excessive” in this context (weights, bias, training epochs) becomes essential when moving on to more complex problems where traditional solutions are no longer sufficient.\n","wordCount":"1650","inLanguage":"en","datePublished":"2025-08-09T00:00:00Z","dateModified":"2025-08-09T00:00:00Z","author":{"@type":"Person","name":"Vincenzo Argese"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://cr1s1um.github.io/en/post/perceptron-c-artificial-neuron-first-step/"},"publisher":{"@type":"Organization","name":"~# cr1s1um \u003e /dev/random","logo":{"@type":"ImageObject","url":"https://cr1s1um.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://cr1s1um.github.io/en/ accesskey=h title="~# cr1s1um > /dev/random (Alt + H)">~# cr1s1um > /dev/random</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://cr1s1um.github.io/it/ title=Italiano aria-label=Italiano>It</a></li></ul></div></div><ul id=menu><li><a href=https://cr1s1um.github.io/en/ title=home><span>home</span></a></li><li><a href=https://cr1s1um.github.io/en/post title=blog><span>blog</span></a></li><li><a href=https://cr1s1um.github.io/en/about-me title="about me"><span>about me</span></a></li><li><a href=https://x.com/cr1s1um title=x><span>x</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://github.com/cr1s1um title=github><span>github</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://cr1s1um.github.io/en/search title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://cr1s1um.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://cr1s1um.github.io/en/post/>Posts</a></div><h1 class="post-title entry-hint-parent">/* Build Your First Artificial Neuron in C: The Perceptron Made Simple */</h1><div class=post-description>Learn how to build a perceptron in C: your first artificial neuron explained step by step, the foundation of neural networks and machine learning.</div><div class=post-meta><span title='2025-08-09 00:00:00 +0000 UTC'>August 9, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Vincenzo Argese</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#perceptron-for-simulating-an-and-logic-gate aria-label="Perceptron for Simulating an AND Logic Gate">Perceptron for Simulating an AND Logic Gate</a><ul><li><a href=#idea aria-label=Idea>Idea</a></li><li><a href=#useful-links aria-label="Useful Links:">Useful Links:</a></li><li><a href=#the-automatic-gate-analogy aria-label="The Automatic Gate Analogy">The Automatic Gate Analogy</a></li><li><a href=#logical-scheme-of-the-example-perceptron aria-label="Logical Scheme of the Example Perceptron">Logical Scheme of the Example Perceptron</a><ul><li><a href=#system-logic-example-from-enkk aria-label="System Logic (Example from @Enkk):">System Logic (Example from @Enkk):</a></li><li><a href=#truth-table-and aria-label="Truth Table (AND):">Truth Table (AND):</a></li></ul></li><li><a href=#connection-with-digital-logic aria-label="Connection with Digital Logic">Connection with Digital Logic</a></li><li><a href=#why-use-a-perceptron aria-label="Why Use a Perceptron?">Why Use a Perceptron?</a></li><li><a href=#what-is-a-perceptron aria-label="What is a Perceptron?">What is a Perceptron?</a><ul><li><a href=#basic-structure aria-label="Basic Structure">Basic Structure</a></li><li><a href=#why-is-the-perceptron-the-smallest-element-of-a-neural-network aria-label="Why is the perceptron the smallest element of a Neural Network?">Why is the perceptron the smallest element of a Neural Network?</a></li><li><a href=#why-is-it-important-to-understand-how-it-works aria-label="Why is it Important to Understand How It Works?">Why is it Important to Understand How It Works?</a></li></ul></li><li><a href=#didactic-value-of-this-exercise aria-label="Didactic Value of This Exercise">Didactic Value of This Exercise</a><ul><li><a href=#1-understanding-the-fundamentals-of-neural-networks aria-label="1. Understanding the Fundamentals of Neural Networks">1. Understanding the Fundamentals of Neural Networks</a></li><li><a href=#2-learning-process aria-label="2. Learning Process">2. Learning Process</a></li><li><a href=#3-learning-algorithm aria-label="3. Learning Algorithm">3. Learning Algorithm</a></li><li><a href=#4-convergence-concepts aria-label="4. Convergence Concepts">4. Convergence Concepts</a></li></ul></li></ul></li><li><a href=#implementation aria-label=Implementation>Implementation</a><ul><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=perceptron-for-simulating-an-and-logic-gate>Perceptron for Simulating an AND Logic Gate<a hidden class=anchor aria-hidden=true href=#perceptron-for-simulating-an-and-logic-gate>#</a></h1><h2 id=idea>Idea<a hidden class=anchor aria-hidden=true href=#idea>#</a></h2><p>This code was developed as a practical exercise based on a YouTube video by <strong>@enkk</strong>.<br>In the video, Enkk explains in a simple and intuitive way what a perceptron is, using a practical and easily understandable example. In this post, we implement the perceptron code.</p><ul><li><a href=https://github.com/cr1s1um/OpenCodeLab/blob/main/c/02_perceptron_and.c>GitHub link to the implementation</a></li></ul><h2 id=useful-links>Useful Links:<a hidden class=anchor aria-hidden=true href=#useful-links>#</a></h2><p><strong>Needless to say, following @Enkk is highly recommended if you are interested in studying Large Language Models (LLMs) and AI.</strong></p><ul><li><a href=https://www.youtube.com/@enkk>Enkk’s YouTube Channel</a></li><li><a href="https://www.youtube.com/watch?v=2UdQQA65jcM&amp;t=2161s">Video “<em>how it works: neural networks (pt.1)</em>”</a></li><li><a href=https://en.wikipedia.org/wiki/Perceptron>Wikipedia - Perceptron</a></li></ul><h2 id=the-automatic-gate-analogy>The Automatic Gate Analogy<a hidden class=anchor aria-hidden=true href=#the-automatic-gate-analogy>#</a></h2><p>The example used in the video is particularly effective: imagine a parking system where a car can enter <strong>only if</strong> both gates (Gate A and Gate B) are open at the same time&mldr; OBVIOUSLY! ;-)</p><h2 id=logical-scheme-of-the-example-perceptron>Logical Scheme of the Example Perceptron<a hidden class=anchor aria-hidden=true href=#logical-scheme-of-the-example-perceptron>#</a></h2><p><img alt="Logical scheme of the example perceptron" loading=lazy src=https://vasystems.it/img/perceptron.webp></p><h3 id=system-logic-example-from-enkk>System Logic (Example from @Enkk):<a hidden class=anchor aria-hidden=true href=#system-logic-example-from-enkk>#</a></h3><ul><li><strong>Gate A open</strong> = signal 1</li><li><strong>Gate B open</strong> = signal 1</li><li><strong>Output 1</strong> = The car may enter</li><li><strong>Output 0</strong> = The car may not enter</li></ul><h3 id=truth-table-and>Truth Table (AND):<a hidden class=anchor aria-hidden=true href=#truth-table-and>#</a></h3><table><thead><tr><th>Gate A</th><th>Gate B</th><th>Result</th><th>Action</th></tr></thead><tbody><tr><td>0 (closed)</td><td>0 (closed)</td><td>0</td><td>❌ Car does not enter</td></tr><tr><td>1 (open)</td><td>0 (closed)</td><td>0</td><td>❌ Car does not enter</td></tr><tr><td>0 (closed)</td><td>1 (open)</td><td>0</td><td>❌ Car does not enter</td></tr><tr><td>1 (open)</td><td>1 (open)</td><td>1</td><td>✅ Car may enter</td></tr></tbody></table><h2 id=connection-with-digital-logic>Connection with Digital Logic<a hidden class=anchor aria-hidden=true href=#connection-with-digital-logic>#</a></h2><p>This analogy perfectly represents the behavior of an <strong>AND logic gate</strong>, where the output is 1 only when both inputs are 1. In all other cases, the output is 0.</p><h2 id=why-use-a-perceptron>Why Use a Perceptron?<a hidden class=anchor aria-hidden=true href=#why-use-a-perceptron>#</a></h2><p>It’s important to note that implementing a simple AND gate requires only a few lines of code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=nf>porta_and</span><span class=p>(</span><span class=kt>int</span> <span class=n>a</span><span class=p>,</span> <span class=kt>int</span> <span class=n>b</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>a</span> <span class=o>&amp;&amp;</span> <span class=n>b</span><span class=p>;</span>  <span class=c1>// Very simple!
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p>&mldr;even simpler:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;stdio.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>(</span><span class=kt>void</span><span class=p>){</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>a</span><span class=p>,</span><span class=n>b</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>a</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span>    <span class=c1>// Assign value 1 or 0
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>b</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span>    <span class=c1>// Assign value 1 or 0
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>a</span> <span class=o>&amp;&amp;</span> <span class=n>b</span><span class=p>){</span>
</span></span><span class=line><span class=cl>        <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Valore output: 1</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Valore output: 0</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h2 id=what-is-a-perceptron>What is a Perceptron?<a hidden class=anchor aria-hidden=true href=#what-is-a-perceptron>#</a></h2><p>The <strong>perceptron</strong> is the simplest and most fundamental form of an artificial neural network. It is a single artificial neuron that makes decisions based on the data it receives as input.</p><h3 id=basic-structure>Basic Structure<a hidden class=anchor aria-hidden=true href=#basic-structure>#</a></h3><p>A perceptron is made up of:</p><ul><li><strong>Inputs</strong>: The data it receives (in our case, the state of the two gates)</li><li><strong>Weights</strong>: Numbers that determine the importance of each input (<strong>VERY IMPORTANT</strong>: weights are the <strong>parameters</strong> of a Large Language Model, i.e., its intelligence)</li><li><strong>Bias</strong>: A threshold value that helps with the final decision</li><li><strong>Activation Function</strong>: Decides whether to “activate” the neuron (output 1) or not (output 0)</li><li><strong>Output</strong>: The final decision (1 or 0)</li></ul><h3 id=why-is-the-perceptron-the-smallest-element-of-a-neural-network>Why is the perceptron the smallest element of a Neural Network?<a hidden class=anchor aria-hidden=true href=#why-is-the-perceptron-the-smallest-element-of-a-neural-network>#</a></h3><p>The perceptron is like the “brick” used to build more complex neural networks:</p><ul><li><strong>Simple Neural Network</strong>: 1 perceptron or a few perceptrons (nowadays only for didactic purposes)</li><li><strong>Multilayer Neural Network</strong>: Hundreds or thousands of perceptrons connected together</li><li><strong>Deep Learning</strong>: Millions of perceptrons organized into layers</li></ul><p>Just as you need to understand how a single brick works before building a house, to understand neural networks it’s useful to first master the perceptron.</p><h3 id=why-is-it-important-to-understand-how-it-works>Why is it Important to Understand How It Works?<a hidden class=anchor aria-hidden=true href=#why-is-it-important-to-understand-how-it-works>#</a></h3><ol><li><strong>Theoretical Foundation</strong>: All advanced concepts (backpropagation, gradient descent, etc.) are derived from perceptron principles.</li><li><strong>Didactic Simplicity</strong>: Simple enough to fully understand, but contains all the key concepts and allows you to build a mental model.</li><li><strong>Historical Basis</strong>: It was the first artificial neuron model.</li><li><strong>Intuitive Understanding</strong>: Once you understand the perceptron, the conceptual leap to more complex networks becomes much more natural.</li></ol><hr><h2 id=didactic-value-of-this-exercise>Didactic Value of This Exercise<a hidden class=anchor aria-hidden=true href=#didactic-value-of-this-exercise>#</a></h2><p>Using a perceptron for this example may seem excessive, but it represents an <strong>excellent educational exercise</strong> for several reasons:</p><ul><li><strong>Simple Problem</strong>: The AND gate is easy to visualize.</li><li><strong>Verifiable Result</strong>: It’s easy to check whether it works (I added printouts in the code step by step).</li><li><strong>Complete Concepts</strong>: Includes all fundamental elements (training, weights, bias, epochs).</li><li><strong>Conceptual Bridge</strong>: Links basic digital logic to advanced machine learning.</li></ul><h3 id=1-understanding-the-fundamentals-of-neural-networks>1. <strong>Understanding the Fundamentals of Neural Networks</strong><a hidden class=anchor aria-hidden=true href=#1-understanding-the-fundamentals-of-neural-networks>#</a></h3><ul><li><strong>Weights</strong>: How the network assigns importance to inputs</li><li><strong>Bias</strong>: The threshold term that influences the final decision</li><li><strong>Activation Function</strong>: How binary outputs are produced</li></ul><h3 id=2-learning-process>2. <strong>Learning Process</strong><a hidden class=anchor aria-hidden=true href=#2-learning-process>#</a></h3><ul><li><strong>Training Epochs</strong>: Repeated learning cycles</li><li><strong>Dataset</strong>: Collection of input-output examples for training</li><li><strong>Weight Updates</strong>: How the network “learns” from its mistakes (<strong>VERY IMPORTANT</strong>)</li></ul><h3 id=3-learning-algorithm>3. <strong>Learning Algorithm</strong><a hidden class=anchor aria-hidden=true href=#3-learning-algorithm>#</a></h3><ul><li><strong>Forward Pass</strong>: Prediction calculation</li><li><strong>Error Calculation</strong>: Comparing with the expected output</li><li><strong>Backward Pass</strong>: Updating parameters based on error</li></ul><h3 id=4-convergence-concepts>4. <strong>Convergence Concepts</strong><a hidden class=anchor aria-hidden=true href=#4-convergence-concepts>#</a></h3><ul><li>How the network reaches an optimal solution</li><li>Stopping criteria for training</li><li>Performance evaluation</li></ul><h1 id=implementation>Implementation<a hidden class=anchor aria-hidden=true href=#implementation>#</a></h1><p><strong>NOTE:</strong> This is version 1 of the implementation. I will probably update the code on GitHub as I come up with new ideas. Always refer to the GitHub project for the latest version.</p><p><a href=https://github.com/cr1s1um/OpenCodeLab/blob/main/c/02_perceptron_and.c>GitHub link to the implementation</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=cm>/*
</span></span></span><span class=line><span class=cl><span class=cm> * PERCEPTRON
</span></span></span><span class=line><span class=cl><span class=cm> * (AND Logic Gate Learning)
</span></span></span><span class=line><span class=cl><span class=cm> * 
</span></span></span><span class=line><span class=cl><span class=cm> * Author: Vincenzo Argese
</span></span></span><span class=line><span class=cl><span class=cm> * Web: https://cr1s1um.github.io/
</span></span></span><span class=line><span class=cl><span class=cm> * Date: 2025-08-09
</span></span></span><span class=line><span class=cl><span class=cm> * Version: v1.0
</span></span></span><span class=line><span class=cl><span class=cm> * Idea: Youtuber @Enkk video “come funziona: le reti neurali (pt.1)” https://www.youtube.com/watch?v=2UdQQA65jcM
</span></span></span><span class=line><span class=cl><span class=cm> * 
</span></span></span><span class=line><span class=cl><span class=cm> * Description:
</span></span></span><span class=line><span class=cl><span class=cm> * This program implements a simple perceptron (single artificial neuron) that learns
</span></span></span><span class=line><span class=cl><span class=cm> * to simulate an AND logic gate through supervised learning.
</span></span></span><span class=line><span class=cl><span class=cm> * 
</span></span></span><span class=line><span class=cl><span class=cm> * The perceptron receives two binary inputs (0 or 1) and learns to output:
</span></span></span><span class=line><span class=cl><span class=cm> * - 1 only when both inputs are 1 (like an AND gate)
</span></span></span><span class=line><span class=cl><span class=cm> * - 0 in all other cases
</span></span></span><span class=line><span class=cl><span class=cm> * 
</span></span></span><span class=line><span class=cl><span class=cm> * Training process:
</span></span></span><span class=line><span class=cl><span class=cm> * 1. Initialize weights and bias with small random values
</span></span></span><span class=line><span class=cl><span class=cm> * 2. For each training example, calculate prediction
</span></span></span><span class=line><span class=cl><span class=cm> * 3. Compare prediction with expected output
</span></span></span><span class=line><span class=cl><span class=cm> * 4. If wrong, adjust weights using perceptron learning rule
</span></span></span><span class=line><span class=cl><span class=cm> * 5. Repeat until all predictions are correct or max epochs reached
</span></span></span><span class=line><span class=cl><span class=cm> * 
</span></span></span><span class=line><span class=cl><span class=cm> * This demonstrates fundamental neural network concepts: weights, bias, 
</span></span></span><span class=line><span class=cl><span class=cm> * activation functions, training epochs, and supervised learning.
</span></span></span><span class=line><span class=cl><span class=cm> */</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;stdio.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=cp>#define EPOCHS 100  </span><span class=c1>// Define constant: maximum number of training epochs
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=cm>/* Forward declaration of activation function */</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>activation_function</span><span class=p>(</span><span class=kt>float</span> <span class=n>sum</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>(</span><span class=kt>void</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=cm>/* Training Dataset */</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=p>[</span><span class=mi>4</span><span class=p>][</span><span class=mi>2</span><span class=p>]</span><span class=o>=</span><span class=p>{</span>  <span class=c1>// Input matrix: 4 examples with 2 features each
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=p>{</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>},</span>  <span class=c1>// First example: x1=0, x2=0 -&gt; expected output: 0
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=p>{</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>},</span>  <span class=c1>// Second example: x1=1, x2=0 -&gt; expected output: 0
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=p>{</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>},</span>  <span class=c1>// Third example: x1=0, x2=1 -&gt; expected output: 0
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=p>{</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>}</span>   <span class=c1>// Fourth example: x1=1, x2=1 -&gt; expected output: 1
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>},</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>y</span><span class=p>[</span><span class=mi>4</span><span class=p>]</span><span class=o>=</span><span class=p>{</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>};</span>  <span class=c1>// Expected outputs: represents AND logic function
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>    <span class=cm>/* Weights and Bias initialization */</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span>
</span></span><span class=line><span class=cl>        <span class=n>w1</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>,</span>  <span class=c1>// Weight for first input, initialized to 0.1
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>w2</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>,</span>  <span class=c1>// Weight for second input, initialized to 0.1
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>b</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>;</span>   <span class=c1>// Bias term, initialized to 0.1
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>    <span class=c1>// Learning rate parameter
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span> <span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>;</span>  <span class=c1>// Controls how much weights are adjusted during learning
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>    <span class=cm>/* Training Phase */</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>epoch</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>epoch</span> <span class=o>&lt;</span> <span class=n>EPOCHS</span><span class=p>;</span> <span class=n>epoch</span><span class=o>++</span><span class=p>){</span>  <span class=c1>// Main training loop: up to 100 epochs
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=kt>int</span> <span class=n>errors</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>  <span class=c1>// Counter for errors in current epoch
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>        <span class=c1>// Process all training examples
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=mi>4</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>){</span>  <span class=c1>// Iterate through all 4 examples in dataset
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>            <span class=c1>// DEBUG - Print current training state
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;TRAINING EPOCH: %d</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>epoch</span><span class=p>);</span>  <span class=c1>// Fixed typo: epoch instead of ephoc
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Current errors: %d</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>errors</span><span class=p>);</span>  <span class=c1>// Print current error count
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Input X1: %d - X2: %d</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>],</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>1</span><span class=p>]);</span>  <span class=c1>// Print current inputs
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Expected output Y: %d</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>  <span class=c1>// Print expected output
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;w1: %.1f - w2: %.1f - Bias: %.1f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>w1</span><span class=p>,</span> <span class=n>w2</span><span class=p>,</span> <span class=n>b</span><span class=p>);</span>  <span class=c1>// Print current weights and bias
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Learning Rate: %.1f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>learning_rate</span><span class=p>);</span>  <span class=c1>// Print learning rate
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>            <span class=c1>// Forward pass: calculate weighted sum
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=kt>float</span> <span class=n>weighted_sum</span> <span class=o>=</span> <span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>*</span><span class=n>w1</span> <span class=o>+</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span><span class=o>*</span><span class=n>w2</span><span class=p>)</span> <span class=o>+</span> <span class=n>b</span><span class=p>;</span>  <span class=c1>// Linear combination: x1*w1 + x2*w2 + bias
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=kt>int</span> <span class=n>predicted_output</span> <span class=o>=</span> <span class=nf>activation_function</span><span class=p>(</span><span class=n>weighted_sum</span><span class=p>);</span>   <span class=c1>// Apply activation function
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>            <span class=c1>// DEBUG - Print forward pass results
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Weighted sum: %.1f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>weighted_sum</span><span class=p>);</span>  <span class=c1>// Print the weighted sum
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Predicted output: %d</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>predicted_output</span><span class=p>);</span>  <span class=c1>// Print network&#39;s prediction
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>            <span class=c1>// Calculate prediction error
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=kt>int</span> <span class=n>error</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>predicted_output</span><span class=p>;</span>  <span class=c1>// Error = expected - predicted
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>            <span class=c1>// DEBUG - Print error information
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Prediction error: %d</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>error</span><span class=p>);</span>  <span class=c1>// Print the error
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;---------------------------------------------</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>  <span class=c1>// Separator line
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>            <span class=c1>// Update weights if there&#39;s an error (perceptron learning rule)
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=k>if</span> <span class=p>(</span><span class=n>error</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>){</span>  <span class=c1>// Only update if prediction is wrong
</span></span></span><span class=line><span class=cl><span class=c1></span>                <span class=c1>// Apply perceptron learning rule
</span></span></span><span class=line><span class=cl><span class=c1></span>                <span class=n>w1</span> <span class=o>+=</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=n>error</span> <span class=o>*</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>];</span>  <span class=c1>// Update w1: w1 = w1 + η * error * x1
</span></span></span><span class=line><span class=cl><span class=c1></span>                <span class=n>w2</span> <span class=o>+=</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=n>error</span> <span class=o>*</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>1</span><span class=p>];</span>  <span class=c1>// Update w2: w2 = w2 + η * error * x2
</span></span></span><span class=line><span class=cl><span class=c1></span>                <span class=n>b</span> <span class=o>+=</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=n>error</span><span class=p>;</span>             <span class=c1>// Update bias: b = b + η * error
</span></span></span><span class=line><span class=cl><span class=c1></span>                <span class=n>errors</span><span class=o>++</span><span class=p>;</span>  <span class=c1>// Increment error counter
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1>// Check convergence: stop if no errors occurred
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>if</span><span class=p>(</span><span class=n>errors</span> <span class=o>==</span> <span class=mi>0</span><span class=p>){</span>  <span class=c1>// Perfect classification achieved
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;TRAINING COMPLETED - epoch: %d</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>epoch</span><span class=p>);</span>  <span class=c1>// Print completion message
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=k>break</span><span class=p>;</span>  <span class=c1>// Exit training loop early
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Print final results
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;</span><span class=se>\n\n</span><span class=s>&#34;</span><span class=p>);</span>  <span class=c1>// Print empty lines for readability
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;=== FINAL TRAINED PARAMETERS ===</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>  <span class=c1>// Print results header
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Final w1: %.1f - w2: %.1f - Bias: %.1f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>w1</span><span class=p>,</span> <span class=n>w2</span><span class=p>,</span> <span class=n>b</span><span class=p>);</span>  <span class=c1>// Print final weights and bias
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>    <span class=c1>// Test the trained perceptron
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;</span><span class=se>\n</span><span class=s>=== TESTING TRAINED PERCEPTRON ===</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=mi>4</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>){</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>test_sum</span> <span class=o>=</span> <span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>*</span><span class=n>w1</span> <span class=o>+</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span><span class=o>*</span><span class=n>w2</span><span class=p>)</span> <span class=o>+</span> <span class=n>b</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>test_output</span> <span class=o>=</span> <span class=nf>activation_function</span><span class=p>(</span><span class=n>test_sum</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Input (%d,%d) -&gt; Output: %d (Expected: %d)</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>               <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>],</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>1</span><span class=p>],</span> <span class=n>test_output</span><span class=p>,</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=cm>/* 
</span></span></span><span class=line><span class=cl><span class=cm> * Activation Function
</span></span></span><span class=line><span class=cl><span class=cm> * 
</span></span></span><span class=line><span class=cl><span class=cm> * Purpose: Converts continuous weighted sum to binary output
</span></span></span><span class=line><span class=cl><span class=cm> * Parameter: float sum - the weighted sum from perceptron
</span></span></span><span class=line><span class=cl><span class=cm> * Returns: 1 if sum &gt; 0, otherwise returns 0
</span></span></span><span class=line><span class=cl><span class=cm> * 
</span></span></span><span class=line><span class=cl><span class=cm> * This implements a threshold activation function commonly used in perceptrons
</span></span></span><span class=line><span class=cl><span class=cm> */</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>activation_function</span><span class=p>(</span><span class=kt>float</span> <span class=n>sum</span><span class=p>){</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>sum</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>)</span> <span class=o>?</span> <span class=mi>1</span> <span class=o>:</span> <span class=mi>0</span><span class=p>;</span>  <span class=c1>// Ternary operator: if sum &gt; 0 return 1, else return 0
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>An AND gate can be implemented in a much simpler way, but this exercise provides a <strong>solid foundation for understanding the fundamental mechanisms</strong> behind more complex neural networks. It serves as an ideal bridge between basic digital logic and advanced machine learning concepts.</p><p>Every element that may seem “excessive” in this context (weights, bias, training epochs) becomes <strong>essential</strong> when moving on to more complex problems where traditional solutions are no longer sufficient.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://cr1s1um.github.io/en/tags/programming-languages/>Programming Languages</a></li><li><a href=https://cr1s1um.github.io/en/tags/c/>C</a></li><li><a href=https://cr1s1um.github.io/en/tags/ai/>AI</a></li><li><a href=https://cr1s1um.github.io/en/tags/machine-learning/>Machine Learning</a></li><li><a href=https://cr1s1um.github.io/en/tags/neural-network/>Neural Network</a></li><li><a href=https://cr1s1um.github.io/en/tags/llm/>LLM</a></li><li><a href=https://cr1s1um.github.io/en/tags/large-language-model/>Large Language Model</a></li></ul><nav class=paginav><a class=prev href=https://cr1s1um.github.io/en/post/bannator-python-script-blocks-brute-force/><span class=title>« Prev</span><br><span>/* bannator.py: The 44 line Python Script That Blocks Brute Force Attacks (this script introduces you to programming!) */</span>
</a><a class=next href=https://cr1s1um.github.io/en/post/coding-italy-scratch-vs-real-programming/><span class=title>Next »</span><br><span>/* Coding in Italy: Why Scratch Isn’t Enough (and What Students Really Need) */</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share /* Build Your First Artificial Neuron in C: The Perceptron Made Simple */ on x" href="https://x.com/intent/tweet/?text=%2f%2a%20Build%20Your%20First%20Artificial%20Neuron%20in%20C%3a%20The%20Perceptron%20Made%20Simple%20%2a%2f&amp;url=https%3a%2f%2fcr1s1um.github.io%2fen%2fpost%2fperceptron-c-artificial-neuron-first-step%2f&amp;hashtags=Programminglanguages%2cC%2cAI%2cMachineLearning%2cNeuralNetwork%2cLLM%2cLargeLanguageModel"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share /* Build Your First Artificial Neuron in C: The Perceptron Made Simple */ on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcr1s1um.github.io%2fen%2fpost%2fperceptron-c-artificial-neuron-first-step%2f&amp;title=%2f%2a%20Build%20Your%20First%20Artificial%20Neuron%20in%20C%3a%20The%20Perceptron%20Made%20Simple%20%2a%2f&amp;summary=%2f%2a%20Build%20Your%20First%20Artificial%20Neuron%20in%20C%3a%20The%20Perceptron%20Made%20Simple%20%2a%2f&amp;source=https%3a%2f%2fcr1s1um.github.io%2fen%2fpost%2fperceptron-c-artificial-neuron-first-step%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share /* Build Your First Artificial Neuron in C: The Perceptron Made Simple */ on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcr1s1um.github.io%2fen%2fpost%2fperceptron-c-artificial-neuron-first-step%2f&title=%2f%2a%20Build%20Your%20First%20Artificial%20Neuron%20in%20C%3a%20The%20Perceptron%20Made%20Simple%20%2a%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share /* Build Your First Artificial Neuron in C: The Perceptron Made Simple */ on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcr1s1um.github.io%2fen%2fpost%2fperceptron-c-artificial-neuron-first-step%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share /* Build Your First Artificial Neuron in C: The Perceptron Made Simple */ on whatsapp" href="https://api.whatsapp.com/send?text=%2f%2a%20Build%20Your%20First%20Artificial%20Neuron%20in%20C%3a%20The%20Perceptron%20Made%20Simple%20%2a%2f%20-%20https%3a%2f%2fcr1s1um.github.io%2fen%2fpost%2fperceptron-c-artificial-neuron-first-step%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share /* Build Your First Artificial Neuron in C: The Perceptron Made Simple */ on telegram" href="https://telegram.me/share/url?text=%2f%2a%20Build%20Your%20First%20Artificial%20Neuron%20in%20C%3a%20The%20Perceptron%20Made%20Simple%20%2a%2f&amp;url=https%3a%2f%2fcr1s1um.github.io%2fen%2fpost%2fperceptron-c-artificial-neuron-first-step%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share /* Build Your First Artificial Neuron in C: The Perceptron Made Simple */ on ycombinator" href="https://news.ycombinator.com/submitlink?t=%2f%2a%20Build%20Your%20First%20Artificial%20Neuron%20in%20C%3a%20The%20Perceptron%20Made%20Simple%20%2a%2f&u=https%3a%2f%2fcr1s1um.github.io%2fen%2fpost%2fperceptron-c-artificial-neuron-first-step%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://utteranc.es/client.js repo=cr1s1um/cr1s1um.github.io issue-term=title theme=github-dark crossorigin=anonymous async></script></article></main><footer class=footer><div class=footer-menu><span><a href=/>home</a></span>
<span><a href=/en/archives/>archives</a></span>
<span><a href=/en/privacy/>privacy and cookie policy</a></span></div><span>&copy; 2025 <a href=https://cr1s1um.github.io/en/>~# cr1s1um > /dev/random</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><style>#consent-notice{padding:1rem;display:none;text-align:center;position:fixed;bottom:0;width:calc(100% - 2rem);background:#222;color:#fff}#consent-notice span{margin-right:1rem}#consent-notice button{cursor:pointer;display:inline-block;width:auto}#consent-notice span a{color:inherit;text-decoration:underline;text-decoration-color:#fff}#consent-notice button.btn{margin-left:.5rem;color:#fff}#consent-notice button.btn.manage-consent{background:#5c5c5c;font-weight:400}#consent-overlay{position:fixed;left:0;top:0;width:100%;height:100vh;display:none;background:rgba(0,0,0,.75);z-index:999999;overflow:auto;cursor:pointer}#consent-overlay.active{display:flex}#consent-overlay>div{background:#000;width:100%;max-width:30rem;padding:1.75rem;margin:auto;cursor:initial}#consent-overlay>div>div{display:flex;align-items:flex-start;margin-bottom:1rem}#consent-overlay>div>div:last-child{margin:0}#consent-overlay h3{padding-top:0}#consent-overlay input{margin-top:.3rem}#consent-overlay label{display:block}#consent-overlay .btn{margin-right:.5rem;color:#fff}#consent-overlay button.btn.save-consent{background:#5c5c5c;font-weight:400}#consent-overlay{color:#fff}#consent-overlay h3{color:#fff}@media(max-width:767px){#consent-overlay>div{padding:1.75rem 1rem}#consent-notice span{display:block;padding-top:3px;margin-bottom:1.5rem}#consent-notice button.btn{position:relative;bottom:4px}}</style><div id=consent-notice><span>Questo sito utilizza i cookie per migliorare l'esperienza di utilizzo, leggi la sezione <a class=manage-consent href=/privacy>Privacy e Cookie</a> per maggiori informazioni.</span><button class="btn manage-consent"> /* Gestisci le preferenze */ </gestisci></button><button class="btn deny-consent"> /* Rifiuta */ </button><button class="btn approve-consent"> /* Accetta */</button></div><div id=consent-overlay><div><div><input type=checkbox id=item0 value=1 name=item0>
<label for=item0><h3>Google Anaytics</h3><p>Il sito utilizza Google Analytics (gtag.js) per analizzare in forma aggregata le visite al sito e migliorare i contenuti. I cookie possono raccogliere informazioni come l’indirizzo IP, browser, dispositivo, sistema operativo, pagine visitate. La base giuridica è il consenso dell’interessato (art. 6, par. 1, lett. a GDPR). Il tracciamento è attivo solo previo consenso tramite banner cookie.</p></label></div><div><button id=save-consent class="btn save-consent" data-consentvalue=0> /* Salva le preferenze */ </button><button class="btn approve-consent"> /* Accetta tutto */</button></div></div></div><script>const scripts=[];scripts[0]="/js/gtag.js";function createCookie(e,t,n){var s,o="";n&&(s=new Date,s.setTime(s.getTime()+n*24*60*60*1e3),o="; expires="+s.toUTCString()),document.cookie=e+"="+t+o+"; path=/"}function readCookie(e){for(var t,s=e+"=",o=document.cookie.split(";"),n=0;n<o.length;n++){for(t=o[n];t.charAt(0)==" ";)t=t.substring(1,t.length);if(t.indexOf(s)==0)return t.substring(s.length,t.length)}return null}function eraseCookie(e){createCookie(e,"",-1)}function denyAllConsentScripts(){var e="";scripts.forEach(function(){e=e+"0"}),acceptSomeConsentScripts(e)}function acceptAllConsentScripts(){var e="";scripts.forEach(function(){e=e+"1"}),acceptSomeConsentScripts(e)}function acceptSomeConsentScripts(e){setConsentInputs(e),createCookie("consent-settings",e,31),document.getElementById("consent-notice").style.display="none",document.getElementById("consent-overlay").classList.remove("active"),loadConsentScripts(e)}function loadConsentScripts(e){scripts.forEach(function(t,n){if(e[n]=="1"){var s=document.createElement("script");s.type="text/javascript",s.src=t,document.body.appendChild(s)}})}function setConsentInputs(e){var t=document.querySelectorAll("#consent-overlay input:not([disabled])");t.forEach(function(t,n){e[n]=="1"?t.checked=!0:t.checked=!1})}function setConsentValue(){var t=document.querySelectorAll("#consent-overlay input:not([disabled])"),e="";t.forEach(function(t){t.checked?e=e+"1":e=e+"0"}),document.getElementById("save-consent").dataset.consentvalue=e}var consentValue,elements=document.querySelectorAll("#consent-overlay input:not([disabled])");elements.forEach(function(e){e.checked=!1}),readCookie("consent-settings")?(consentValue=readCookie("consent-settings").toString(),setConsentInputs(consentValue),loadConsentScripts(consentValue)):document.getElementById("consent-notice").style.display="block",elements=document.querySelectorAll(".manage-consent"),elements.forEach(function(e){e.addEventListener("click",function(){document.getElementById("consent-overlay").classList.toggle("active")})}),elements=document.querySelectorAll(".deny-consent"),elements.forEach(function(e){e.addEventListener("click",function(){denyAllConsentScripts()})}),elements=document.querySelectorAll(".approve-consent"),elements.forEach(function(e){e.addEventListener("click",function(){acceptAllConsentScripts()})}),document.getElementById("save-consent").addEventListener("click",function(){setConsentValue(),acceptSomeConsentScripts(this.dataset.consentvalue)}),document.getElementById("consent-overlay").addEventListener("click",function(e){document.querySelector("#consent-overlay > div").contains(e.target)||this.classList.toggle("active")})</script></body></html>